import random
import pandas as pd

def calculate_ldr(df: pd.DataFrame) -> pd.DataFrame:
    """
    å…¬å¼è©•ä¾¡ï¼ˆè¡¨ã®é¡”ï¼‰ã¨AIç®—å‡ºã®å®ŸåŠ¹è©•ä¾¡ï¼ˆçœŸå®Ÿï¼‰ã‚’æ¯”è¼ƒã—ã€
    æƒ…å ±ã®éå¯¾ç§°æ€§ã‚’ã€ŒLDRï¼ˆLie Divergence Rateï¼‰ã€ã¨ã—ã¦æ•°å€¤åŒ–ã™ã‚‹ã€‚
    
    Args:
        df: åº—èˆ—ãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã‚‹DataFrameã€‚'official_rating'ã‚«ãƒ©ãƒ å¿…é ˆã€‚
        
    Returns:
        pd.DataFrame: ldr, status, ai_real_score ãŒè¿½åŠ ã•ã‚ŒãŸDataFrame
    """
    if df.empty:
        return df

    # ã‚³ãƒ”ãƒ¼ã‚’ä½œæˆã—ã¦å…ƒã®DFã¸ã®å‰¯ä½œç”¨ã‚’é˜²ãï¼ˆå®‰å…¨ãªãƒ‡ãƒ¼ã‚¿æ“ä½œï¼‰
    result_df = df.copy()

    # AIæ„Ÿæƒ…åˆ†æã‚¨ãƒ³ã‚¸ãƒ³ (Evidence-Based Semantic Analysis)
    def analyze_sentiment(row):
        # ãƒ™ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢ (å…¬å¼è©•ä¾¡ã‚’å‡ºç™ºç‚¹ã¨ã™ã‚‹)
        base_score = float(row.get('official_rating', 3.0))
        
        # ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹å–å¾—
        leak_text = str(row.get('bakusai_leak', ""))
        official_text = str(row.get('official_review', ""))
        full_text = leak_text + " " + official_text
        
        # è©•ä¾¡èª¿æ•´å¤‰æ•°
        adjustment = 0.0
        
        # 1. ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰èª¿æŸ» (æ¸›ç‚¹)
        # å®Ÿéš›ã«ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒä½¿ã†ç”Ÿã®è¨€è‘‰ã‚’ãƒªã‚¹ãƒˆåŒ–
        negative_signals = [
            "åœ°é›·", "ãƒ–ã‚¹", "ãƒãƒã‚¢", "BBA", "å†™çœŸè©æ¬º", "ãƒ‘ãƒãƒã‚¸", "æ…‹åº¦æ‚ªã„", 
            "é‡‘ãƒ‰ãƒ–", "äºŒåº¦ã¨è¡Œã‹ãªã„", "ã‚´ãƒŸ", "æœ€æ‚ª", "å¾®å¦™", "ãƒã‚ºãƒ¬",
            "ãƒãƒã‚¡", "ä¿®æ­£", "è©æ¬º"
        ]
        
        # 2. ãƒã‚¸ãƒ†ã‚£ãƒ–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰èª¿æŸ» (åŠ ç‚¹/æ•‘æ¸ˆ)
        positive_signals = [
            "ç¥", "ãƒªãƒ”ç¢º", "æœ€é«˜", "å½“ãŸã‚Š", "å¯æ„›ã„", "ã‚ˆã‹ã£ãŸ", 
            "å„ªè‰¯", "ãƒ¬ãƒ™ãƒ«é«˜ã„", "æœ¬ç‰©", "ã‚¨ãƒ­ã„"
        ]
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°
        hit_negatives = 0
        hit_positives = 0
        
        for word in negative_signals:
            if word in full_text:
                hit_negatives += 1
                adjustment -= 0.8 # 1ãƒ¯ãƒ¼ãƒ‰ã”ã¨ã®æ¸›ç‚¹å¹…
                
        for word in positive_signals:
            if word in full_text:
                hit_positives += 1
                adjustment += 0.5 # 1ãƒ¯ãƒ¼ãƒ‰ã”ã¨ã®åŠ ç‚¹å¹…ï¼ˆãƒã‚¬ãƒ†ã‚£ãƒ–ã‚ˆã‚Šé‡ã¿ã¯ä½ã„ï¼‰

        # 3. ãƒªã‚¹ã‚¯ä¿‚æ•° (å…¬å¼è©•ä¾¡ãŒé«˜ã™ãã‚‹å ´åˆã®ã€Œç››ã£ã¦ã‚‹ã€ãƒªã‚¹ã‚¯)
        # å…¬å¼ãŒ4.5ä»¥ä¸Šã§ã€ã‹ã¤ãƒã‚¸ãƒ†ã‚£ãƒ–ãªè£ä»˜ã‘ãŒãªã„å ´åˆã¯æ€ªã—ã„ã¨ã¿ãªã™
        if base_score >= 4.5 and hit_positives == 0:
            adjustment -= 1.0

        # 4. æƒ…å ±ä¸åœ¨ãƒšãƒŠãƒ«ãƒ†ã‚£
        # ãƒªãƒ¼ã‚¯æƒ…å ±ãŒå–ã‚Œãªã‹ã£ãŸå ´åˆã€å°‘ã—å‰²ã‚Šå¼•ãï¼ˆä¸ç¢ºå®Ÿæ€§ï¼‰
        if "ã‚¢ã‚¯ã‚»ã‚¹é®æ–­" in leak_text or "å¤±æ•—" in leak_text or "nan" in leak_text:
            adjustment -= 0.5 

        # ãƒ©ãƒ³ãƒ€ãƒ ãªæºã‚‰ãï¼ˆå€‹äººã®ä¸»è¦³å·®ï¼‰
        uncertainty = random.uniform(-0.3, 0.3)
        
        final_score = base_score + adjustment + uncertainty
        return round(max(0.0, min(5.0, final_score)), 1)

    result_df['ai_real_score'] = result_df.apply(analyze_sentiment, axis=1)
    
    # LDRè¨ˆç®—ãƒ­ã‚¸ãƒƒã‚¯
    # å¼: (|å…¬å¼ - å®ŸåŠ¹| / å…¬å¼) * 100
    # æ„å›³: å˜ãªã‚‹å·®åˆ†ã§ã¯ãªãã€ŒæœŸå¾…å€¤ã«å¯¾ã™ã‚‹è£åˆ‡ã‚Šã®å‰²åˆã€ã‚’é‡è¦–ã™ã‚‹ãŸã‚ã€åˆ†æ¯ã‚’å…¬å¼è©•ä¾¡ã¨ã™ã‚‹ã€‚
    result_df['ldr'] = result_df.apply(lambda x: 
        ((abs(x['official_rating'] - x['ai_real_score']) / x['official_rating'] * 100) 
         if x['official_rating'] > 0 else 0), axis=1).round(1)
    
    # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒ©ãƒ™ãƒ«ä»˜ä¸
    def label_status(ldr):
        if ldr >= 50:
            return 'ğŸ’€ãƒã‚ºãƒ¬ç¢ºå®š' # High Risk
        elif ldr >= 30:
            return 'âš ï¸è¦æ³¨æ„'   # Warning
        else:
            return 'âœ…å„ªè‰¯'     # Safe
            
    result_df['status'] = result_df['ldr'].apply(label_status)
    
    return result_df
