"""
Devil's DX Scraper v2.0 - Strategic Architecture Overhaul
=========================================================
è‡´å‘½çš„æ¬ é™¥5ç‚¹ã‚’ä¿®æ­£ã—ãŸå®Œå…¨å†è¨­è¨ˆç‰ˆã€‚

ä¿®æ­£å†…å®¹:
1. NightHeavené™¤å¤–ï¼ˆ404è§£æ¶ˆï¼‰
2. GoogleçµŒç”±Bakusaiæ¤œç´¢å»ƒæ­¢ â†’ ã‚¨ãƒªã‚¢ãƒ¡ãƒ‹ãƒ¥ãƒ¼ç›´æ¥æ¤œç´¢ï¼ˆreCAPTCHAå›é¿ï¼‰
3. èµ·å‹•å‰pkillã§ã‚¾ãƒ³ãƒ“ãƒ—ãƒ­ã‚»ã‚¹é§†é€
4. finallyå¥ã§ç¢ºå®Ÿã«context.close()
5. éƒ¨åˆ†çš„æˆåŠŸãƒ‡ãƒ¼ã‚¿ã‚‚è¿”å´å¯èƒ½ã«
"""

import pandas as pd
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup
import time
import random

import urllib.parse

# ã‚¿ãƒ¼ã‚²ãƒƒãƒˆURLå®šç¾©ï¼ˆNightHeavené™¤å¤– - 404è§£æ¶ˆï¼‰
TARGET_URLS = {
    "ã‚½ãƒ¼ãƒ—": "https://www.cityheaven.net/tochigi/A0901/A090101/shop-list/biz4/",
    "ãƒ‡ãƒªãƒ˜ãƒ«": "https://www.cityheaven.net/tochigi/A0901/A090101/shop-list/biz6/",
    "ãƒ¡ãƒ³ã‚¨ã‚¹": "https://www.cityheaven.net/tochigi/A0901/A090101/shop-list/biz7/",
}

# Bakusaiã‚¨ãƒªã‚¢ã‚³ãƒ¼ãƒ‰ï¼ˆåŒ—é–¢æ± = æ ƒæœ¨/å®‡éƒ½å®®å«ã‚€ï¼‰
BAKUSAI_AREA_CODE = 15


def _kill_zombie_chromium():
    """
    èµ·å‹•å‰ã«ã‚¾ãƒ³ãƒ“Chromiumãƒ—ãƒ­ã‚»ã‚¹ã‚’é§†é€ï¼ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ­ãƒƒã‚¯å›é¿ï¼‰
    
    æ³¨æ„: pkill chromiumã¯ä»–ã®Chromiumãƒ—ãƒ­ã‚»ã‚¹ï¼ˆAntigravityãƒ–ãƒ©ã‚¦ã‚¶ç­‰ï¼‰ã‚‚æ®ºã™å±é™ºãŒã‚ã‚‹ã€‚
    ä»£ã‚ã‚Šã«user_data_dirã®ãƒ­ãƒƒã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã€å¿…è¦ã«å¿œã˜ã¦å‰Šé™¤ã™ã‚‹ã€‚
    """
    import os
    import shutil
    
    user_data_dir = "./user_data_dir"
    
    # ãƒ­ãƒƒã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆChromiumãŒä½¿ç”¨ä¸­ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä½œæˆã•ã‚Œã‚‹ï¼‰
    lock_files = [
        os.path.join(user_data_dir, "SingletonLock"),
        os.path.join(user_data_dir, "SingletonCookie"),
        os.path.join(user_data_dir, "SingletonSocket"),
    ]
    
    try:
        for lock_file in lock_files:
            if os.path.exists(lock_file):
                try:
                    os.remove(lock_file)
                    print(f"ğŸ§¹ Removed stale lock: {lock_file}")
                except Exception as e:
                    print(f"âš ï¸ Could not remove {lock_file}: {e}")
        
        print("ğŸ§¹ Lock cleanup completed.")
    except Exception as e:
        print(f"âš ï¸ Cleanup warning (non-fatal): {e}")


def _search_bakusai_direct(page, store_name: str) -> str:
    """
    Bakusaiã‚¨ãƒªã‚¢ãƒ¡ãƒ‹ãƒ¥ãƒ¼çµŒç”±ã§æ¤œç´¢ï¼ˆGoogleå®Œå…¨ãƒã‚¤ãƒ‘ã‚¹ï¼‰
    
    æˆ¦ç•¥:
    1. ã‚¨ãƒªã‚¢ãƒ¡ãƒ‹ãƒ¥ãƒ¼ãƒ•ãƒ¬ãƒ¼ãƒ ã«ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹
    2. JavaScriptæ³¨å…¥ã§æ¤œç´¢å®Ÿè¡Œ
    3. æ¤œç´¢çµæœã‹ã‚‰ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’å–å¾—
    4. ã‚¹ãƒ¬ãƒƒãƒ‰ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’æŠ½å‡º
    
    Returns:
        str: æŠ½å‡ºã—ãŸã‚³ãƒ¡ãƒ³ãƒˆï¼ˆå¤±æ•—æ™‚ã¯ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼‰
    """
    try:
        # Step 1: ã‚¨ãƒªã‚¢ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã«ã‚¢ã‚¯ã‚»ã‚¹
        menu_url = f"https://bakusai.com/areamenu/acode={BAKUSAI_AREA_CODE}/"
        print(f"  ğŸ“¡ Bakusaiç›´æ¥æ¤œç´¢: {store_name}")
        page.goto(menu_url, timeout=30000, wait_until="domcontentloaded")
        time.sleep(2)
        
        # Step 2: JavaScriptæ³¨å…¥ã§æ¤œç´¢å®Ÿè¡Œ
        search_script = f"""
        (() => {{
            const input = document.getElementById('idWord');
            if (input) {{
                input.value = '{store_name} å®‡éƒ½å®®';
                const button = document.getElementById('schWordsSubmit');
                if (button) {{
                    button.click();
                    return 'searched';
                }}
            }}
            return 'input_not_found';
        }})()
        """
        result = page.evaluate(search_script)
        
        if result == 'input_not_found':
            return "æ¤œç´¢ãƒ•ã‚©ãƒ¼ãƒ æœªæ¤œå‡º"
        
        # Step 3: æ¤œç´¢çµæœãƒšãƒ¼ã‚¸ã®èª­ã¿è¾¼ã¿å¾…æ©Ÿ
        time.sleep(3)
        page.wait_for_load_state("domcontentloaded", timeout=15000)
        
        # Step 4: ã‚¹ãƒ¬ãƒƒãƒ‰ãƒªãƒ³ã‚¯ã‚’æ¢ã™
        thread_links = page.locator("a[href*='/thr_res/']").all()
        
        if not thread_links:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: sch_allãƒšãƒ¼ã‚¸ã«ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹
            encoded_query = urllib.parse.quote(f"{store_name} å®‡éƒ½å®®")
            fallback_url = f"https://bakusai.com/sch_all/acode={BAKUSAI_AREA_CODE}/word={encoded_query}/"
            page.goto(fallback_url, timeout=30000)
            time.sleep(2)
            thread_links = page.locator("a[href*='/thr_res/']").all()
        
        if not thread_links:
            return "ã‚¹ãƒ¬ãƒƒãƒ‰æœªç™ºè¦‹"
        
        # Step 5: æœ€åˆã®ã‚¹ãƒ¬ãƒƒãƒ‰ã«ã‚¢ã‚¯ã‚»ã‚¹
        first_thread = thread_links[0]
        href = first_thread.get_attribute("href")
        
        if href:
            thread_url = f"https://bakusai.com{href}" if href.startswith("/") else href
            print(f"    â†’ ã‚¹ãƒ¬ãƒƒãƒ‰ç™ºè¦‹: {href[:50]}...")
            page.goto(thread_url, timeout=30000)
            time.sleep(2)
            
            # Cloudflareãƒã‚§ãƒƒã‚¯
            if "challenge" in page.title().lower() or "attention" in page.title().lower():
                print("    âš ï¸ Cloudflareæ¤œå‡º - æ‰‹å‹•è§£æ±ºå¾…ã¡")
                time.sleep(10)
            
            # Step 6: ã‚³ãƒ¡ãƒ³ãƒˆæŠ½å‡º
            comment_selectors = [
                "div[class*='response_body']",
                "div[class*='article_body']",
                ".comment_text",
                "article",
            ]
            
            raw_texts = []
            for selector in comment_selectors:
                elements = page.locator(selector).all()
                if elements:
                    for el in elements[-15:]:  # æœ€æ–°15ä»¶
                        try:
                            txt = el.inner_text()
                            if len(txt) > 5:
                                raw_texts.append(txt)
                        except:
                            pass
                    if raw_texts:
                        break
            
            if not raw_texts:
                # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: bodyå…¨ä½“ã‹ã‚‰æŠ½å‡º
                try:
                    body_text = page.locator("body").inner_text()
                    raw_texts = [body_text[-1500:]]
                except:
                    pass
            
            if raw_texts:
                full_leak = " || ".join(raw_texts)
                truncated = full_leak[:600] + "..." if len(full_leak) > 600 else full_leak
                print(f"    âœ… {len(raw_texts)}ä»¶ã®ã‚³ãƒ¡ãƒ³ãƒˆå–å¾—")
                return truncated
        
        return "ã‚¹ãƒ¬ãƒƒãƒ‰å†…å®¹å–å¾—å¤±æ•—"
        
    except Exception as e:
        print(f"    âŒ Bakusaiæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        return f"ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—: {str(e)[:50]}"


def fetch_yokohama_data() -> pd.DataFrame:
    """
    å®‡éƒ½å®®ã‚¨ãƒªã‚¢ã®åº—èˆ—ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ãƒ»åˆ†æã™ã‚‹ãƒ¡ã‚¤ãƒ³é–¢æ•°ã€‚
    
    ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ v2.0:
    - Phase 0: ã‚¾ãƒ³ãƒ“ãƒ—ãƒ­ã‚»ã‚¹é§†é€
    - Phase 1: CityHeavenå…¬å¼ãƒ‡ãƒ¼ã‚¿åé›†
    - Phase 2: Bakusaiç›´æ¥æ¤œç´¢ï¼ˆGoogleå®Œå…¨ãƒã‚¤ãƒ‘ã‚¹ï¼‰
    """
    all_stores = []
    context = None
    
    # Phase 0: ãƒ—ãƒ¬ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    _kill_zombie_chromium()
    
    try:
        with sync_playwright() as p:
            print("ğŸ¯ Devil's DX Sniper v2.0 - Launching...")
            
            user_data_dir = "./user_data_dir"
            
            # Persistent Contextèµ·å‹•ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆçŸ­ç¸®ã§ãƒ•ã‚§ã‚¤ãƒ«ãƒ•ã‚¡ã‚¹ãƒˆï¼‰
            context = p.chromium.launch_persistent_context(
                user_data_dir=user_data_dir,
                headless=False,
                slow_mo=50,
                args=["--disable-blink-features=AutomationControlled"],
                user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
                viewport={'width': 1280, 'height': 800},
                locale='ja-JP',
                ignore_https_errors=True,
                timeout=15000
            )
            
            # WebDriverå½è£…
            context.add_init_script("""
                Object.defineProperty(navigator, 'webdriver', {
                    get: () => undefined
                });
            """)
            
            page = context.new_page()
            age_verified = False
            
            # === Phase 1: CityHeavenå…¬å¼ãƒ‡ãƒ¼ã‚¿åé›† ===
            print("\nğŸ“Š Phase 1: CityHeaven Data Collection")
            for category, url in TARGET_URLS.items():
                print(f"  ğŸ¯ {category}: {url}")
                try:
                    page.goto(url, timeout=60000, wait_until="domcontentloaded")
                    
                    # å¹´é½¢ç¢ºèªçªç ´
                    if not age_verified:
                        for selector in [".heavenbutton", "a.btn-enter", "a:has-text('Enter')"]:
                            try:
                                if page.locator(selector).is_visible(timeout=2000):
                                    page.click(selector)
                                    age_verified = True
                                    page.wait_for_load_state("domcontentloaded")
                                    time.sleep(1)
                                    break
                            except:
                                pass
                    
                    time.sleep(2)
                    
                    # åº—èˆ—ãƒªã‚¹ãƒˆè§£æ
                    html = page.content()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    items = soup.select('li')
                    shop_items = [
                        i for i in items 
                        if ("shop" in " ".join(i.get("class", [])) or "list" in " ".join(i.get("class", []))) 
                        and (i.find('a') and (i.find('img') or "å£ã‚³ãƒŸ" in i.text))
                    ]
                    
                    if len(shop_items) < 3:
                        shop_items = [
                            i for i in soup.select('div') 
                            if "shop_list" in " ".join(i.get("class", [])) or "shop-item" in " ".join(i.get("class", []))
                        ]
                    
                    category_items = shop_items[:10]  # å„ã‚«ãƒ†ã‚´ãƒªæœ€å¤§10åº—èˆ—
                    
                    for item in category_items:
                        try:
                            name = ""
                            for sel in ['a.shop_title_shop', '.shop-name', 'span[itemprop="name"]', 'h2 a', 'h3 a', '.shop_name a']:
                                el = item.select_one(sel)
                                if el and el.get_text(strip=True):
                                    name = el.get_text(strip=True)
                                    break
                            
                            if not name or "æ±‚äºº" in name:
                                continue
                            
                            # è©•ä¾¡å–å¾—
                            rating = 0.0
                            stars = item.select('img[src*="star"]')
                            if stars:
                                real_stars = [s for s in stars if 'on' in s.get('src', '') or 'gold' in s.get('src', '')]
                                if real_stars:
                                    rating = float(len(real_stars))
                            
                            # å…¬å¼å£ã‚³ãƒŸã‚µãƒ³ãƒ—ãƒ«
                            official_review = ""
                            review_elem = item.select_one('.shop_comment') or item.select_one('.comment_body') or item.select_one('.review_text')
                            if review_elem:
                                official_review = review_elem.get_text(strip=True)[:50] + "..."
                            
                            all_stores.append({
                                "name": name,
                                "official_rating": rating,
                                "official_review": official_review,
                                "category": category,
                                "bakusai_leak": ""  # Phase 2ã§åŸ‹ã‚ã‚‹
                            })
                            
                        except Exception as e:
                            continue
                    
                    print(f"    âœ… {len(category_items)} shops found")
                    
                except Exception as e:
                    print(f"    âŒ Error: {e}")
                    continue
            
            # === Phase 2: Bakusaiç›´æ¥æ¤œç´¢ ===
            print("\nğŸ•µï¸ Phase 2: Bakusai Intelligence (Direct Search)")
            
            # å„ã‚«ãƒ†ã‚´ãƒªã‹ã‚‰ä¸Šä½2åº—èˆ—ã‚’æ·±å €ã‚Š
            deep_targets = []
            cat_counts = {}
            for store in all_stores:
                cat = store['category']
                if cat not in cat_counts:
                    cat_counts[cat] = 0
                if cat_counts[cat] < 2:
                    deep_targets.append(store)
                    cat_counts[cat] += 1
            
            for store in deep_targets:
                leak = _search_bakusai_direct(page, store['name'])
                store['bakusai_leak'] = leak
                time.sleep(random.uniform(2, 4))  # ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆå¯¾ç­–
            
            print("\nâœ… Data collection complete.")
            return pd.DataFrame(all_stores)
    
    except Exception as e:
        print(f"âŒ Critical error: {e}")
        # éƒ¨åˆ†çš„æˆåŠŸãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°è¿”ã™
        if all_stores:
            print(f"âš ï¸ Returning partial data ({len(all_stores)} stores)")
            return pd.DataFrame(all_stores)
        return pd.DataFrame()
    
    finally:
        # ç¢ºå®Ÿã«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒ­ãƒ¼ã‚ºï¼ˆæ¬ é™¥4ä¿®æ­£ï¼‰
        if context:
            try:
                context.close()
                print("ğŸ”’ Browser context closed.")
            except Exception as e:
                print(f"âš ï¸ Context close warning: {e}")


